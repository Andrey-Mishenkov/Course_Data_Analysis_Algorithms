{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale(x):\n",
    "    res = (x - x.mean(axis = 0)) / x.std(axis = 0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "m3d93Gx2dfJ6",
    "outputId": "eaee1c83-f56e-474c-a5f2-bdd5d4659ab4"
   },
   "outputs": [],
   "source": [
    "X, y = datasets.load_iris(return_X_y = True)\n",
    "X = iris.data\n",
    "# X.shape\n",
    "\n",
    "X = X.astype(float)\n",
    "X = standard_scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучить любую модель классификации на датасете IRIS до применения PCA (2 компоненты) и после него. Сравнить качество классификации по отложенной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция понижения размерности методом PCA\n",
    "def get_new_features_PCA(X, n_features):\n",
    "    \n",
    "    # Найдем собственные векторы и собственные значения\n",
    "    #------------------------------------------------------------------------------------------------------------ \n",
    "    covariance_matrix = X.T @ X\n",
    "\n",
    "    eig_values, eig_vectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "    # сформируем список кортежей (собственное значение, собственный вектор)\n",
    "    eig_pairs = [(np.abs(eig_values[i]), eig_vectors[:, i]) for i in range(len(eig_values))]\n",
    "\n",
    "    # и отсортируем список по убыванию собственных значений\n",
    "    eig_pairs.sort(key=lambda x: x[0], reverse = True)\n",
    "\n",
    "    print('\\nСобственные значения и собственные векторы в порядке убывания:')\n",
    "    for i in eig_pairs:\n",
    "        print(i)\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "    # Оценим долю дисперсии, которая описывается найденными компонентами.\n",
    "    eig_sum = sum(eig_values)\n",
    "    var_exp = [(i / eig_sum) * 100 for i in sorted(eig_values, reverse = True)]\n",
    "    cum_var_exp = np.cumsum(var_exp)\n",
    "    \n",
    "    print(f'\\nДоля дисперсии, описываемая каждой из компонент \\n{var_exp}')\n",
    "\n",
    "    # а теперь оценим кумулятивную (то есть накапливаемую) дисперсию при учитывании каждой из компонент\n",
    "    print(f'\\nКумулятивная доля дисперсии по компонентам \\n{cum_var_exp}')\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "    # Сформируем вектор весов из собственных векторов, соответствующих первым двум главным компонентам\n",
    "    W = np.hstack([eig_pairs[i][1].reshape(4, 1) for i in range(n_features)])\n",
    "\n",
    "    print(f'\\nМатрица весов W:\\n', W)\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "    #print(f'\\nX\\n', X[:10, :])\n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "    # Сформируем новую матрицу \"объекты-признаки\"\n",
    "    Z = X.dot(W)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 признака\n",
      "\tAccuracy Train = 0.95\n",
      "\tAccuracy Test  = 0.9333\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------------------\n",
    "# Модель на четырех признаках\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "model = KNeighborsRegressor(3)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_preds = model.predict(X_train).astype(int)\n",
    "y_test_preds  = model.predict(X_test).astype(int)\n",
    "\n",
    "print('4 признака')\n",
    "accuracy_4_train = round(accuracy_score(y_train, y_train_preds), 4)\n",
    "accuracy_4_test  = round(accuracy_score(y_test, y_test_preds), 4)\n",
    "\n",
    "print(f'\\tAccuracy Train =', accuracy_4_train)\n",
    "print(f'\\tAccuracy Test  =', accuracy_4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Собственные значения и собственные векторы в порядке убывания:\n",
      "(437.77467247979894, array([ 0.52106591, -0.26934744,  0.5804131 ,  0.56485654]))\n",
      "(137.10457072021055, array([-0.37741762, -0.92329566, -0.02449161, -0.06694199]))\n",
      "(22.01353133569725, array([-0.71956635,  0.24438178,  0.14212637,  0.63427274]))\n",
      "(3.1072254642929513, array([ 0.26128628, -0.12350962, -0.80144925,  0.52359713]))\n",
      "\n",
      "Доля дисперсии, описываемая каждой из компонент \n",
      "[72.96244541329986, 22.85076178670177, 3.668921889282877, 0.5178709107154922]\n",
      "\n",
      "Кумулятивная доля дисперсии по компонентам \n",
      "[ 72.96244541  95.8132072   99.48212909 100.        ]\n",
      "\n",
      "Матрица весов W:\n",
      " [[ 0.52106591 -0.37741762]\n",
      " [-0.26934744 -0.92329566]\n",
      " [ 0.5804131  -0.02449161]\n",
      " [ 0.56485654 -0.06694199]]\n"
     ]
    }
   ],
   "source": [
    "# Понижение размерности до двух признаков\n",
    "Z_pca = get_new_features_PCA(X, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 признака\n",
      "\tAccuracy Train = 0.8833\n",
      "\tAccuracy Test  = 0.8667\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------------------\n",
    "# Модель на двух признаках\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(Z_pca, y, test_size = 0.2, random_state = 21)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_preds = model.predict(X_train).astype(int)\n",
    "y_test_preds  = model.predict(X_test).astype(int)\n",
    "\n",
    "print('2 признака')\n",
    "accuracy_2_train = round(accuracy_score(y_train, y_train_preds), 4)\n",
    "accuracy_2_test  = round(accuracy_score(y_test, y_test_preds), 4)\n",
    "\n",
    "print(f'\\tAccuracy Train =', accuracy_2_train)\n",
    "print(f'\\tAccuracy Test  =', accuracy_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод\n",
    "После понижения размерности с четырех до двух признаков качество модели на тестовой выборке снижается с 93% до 86%, но в целом остается достаточно хорошим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Написать свою реализацию метода главных компонент с помощью сингулярного разложения с использованием функции numpy.linalg.svd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_iris(return_X_y = True)\n",
    "X = iris.data\n",
    "# X.shape\n",
    "\n",
    "X = X.astype(float)\n",
    "X = standard_scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция понижения размерности методом SVD\n",
    "def get_new_features_SVD(X, n_features):\n",
    "    \n",
    "    # Найдем собственные векторы и собственные значения\n",
    "    #------------------------------------------------------------------------------------------------------------ \n",
    "    covariance_matrix = X.T @ X\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------ \n",
    "    # разложение матрицы ковариации\n",
    "    U, D, VT = np.linalg.svd(covariance_matrix)\n",
    "    \n",
    "    eig_values  = D\n",
    "    eig_vectors = VT.T\n",
    "    #------------------------------------------------------------------------------------------------------------ \n",
    "\n",
    "    # сформируем список кортежей (собственное значение, собственный вектор)\n",
    "    eig_pairs = [(np.abs(eig_values[i]), eig_vectors[:, i]) for i in range(len(eig_values))]\n",
    "\n",
    "    # и отсортируем список по убыванию собственных значений\n",
    "    eig_pairs.sort(key=lambda x: x[0], reverse = True)\n",
    "\n",
    "    print('\\nСобственные значения и собственные векторы в порядке убывания:')\n",
    "    for i in eig_pairs:\n",
    "        print(i)\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "    # Оценим долю дисперсии, которая описывается найденными компонентами.\n",
    "    eig_sum = sum(eig_values)\n",
    "    var_exp = [(i / eig_sum) * 100 for i in sorted(eig_values, reverse = True)]\n",
    "    cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "    print(f'\\nДоля дисперсии, описываемая каждой из компонент \\n{var_exp}')\n",
    "\n",
    "    # а теперь оценим кумулятивную (то есть накапливаемую) дисперсию при учитывании каждой из компонент\n",
    "    print(f'\\nКумулятивная доля дисперсии по компонентам \\n{cum_var_exp}')\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "    # Сформируем вектор весов из собственных векторов, соответствующих первым двум главным компонентам\n",
    "    W = np.hstack([eig_pairs[i][1].reshape(4, 1) for i in range(n_features)])\n",
    "\n",
    "    print(f'\\nМатрица весов W:\\n', W)\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "    #print(f'\\nX\\n', X[:10, :])\n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "    # Сформируем новую матрицу \"объекты-признаки\"\n",
    "    Z = X.dot(W)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Собственные значения и собственные векторы в порядке убывания:\n",
      "(437.77467247979934, array([-0.52106591,  0.26934744, -0.5804131 , -0.56485654]))\n",
      "(137.10457072021055, array([-0.37741762, -0.92329566, -0.02449161, -0.06694199]))\n",
      "(22.013531335697266, array([ 0.71956635, -0.24438178, -0.14212637, -0.63427274]))\n",
      "(3.107225464292947, array([ 0.26128628, -0.12350962, -0.80144925,  0.52359713]))\n",
      "\n",
      "Доля дисперсии, описываемая каждой из компонент \n",
      "[72.96244541329987, 22.850761786701753, 3.668921889282877, 0.5178709107154911]\n",
      "\n",
      "Кумулятивная доля дисперсии по компонентам \n",
      "[ 72.96244541  95.8132072   99.48212909 100.        ]\n",
      "\n",
      "Матрица весов W:\n",
      " [[-0.52106591 -0.37741762]\n",
      " [ 0.26934744 -0.92329566]\n",
      " [-0.5804131  -0.02449161]\n",
      " [-0.56485654 -0.06694199]]\n"
     ]
    }
   ],
   "source": [
    "# Понижение размерности до двух признаков\n",
    "Z_svd = get_new_features_SVD(X, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 признака\n",
      "\tAccuracy Train = 0.8833\n",
      "\tAccuracy Test  = 0.8667\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------------------\n",
    "# Проверка Модели на двух признаках\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(Z_svd, y, test_size = 0.2, random_state = 21)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_preds = model.predict(X_train).astype(int)\n",
    "y_test_preds  = model.predict(X_test).astype(int)\n",
    "\n",
    "print('2 признака')\n",
    "accuracy_2_train = round(accuracy_score(y_train, y_train_preds), 4)\n",
    "accuracy_2_test  = round(accuracy_score(y_test, y_test_preds), 4)\n",
    "\n",
    "print(f'\\tAccuracy Train =', accuracy_2_train)\n",
    "print(f'\\tAccuracy Test  =', accuracy_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод \n",
    "Значения Accuracy для train и для test для двух признаков получились такие же, как и в задании 1.\n",
    "\n",
    "Оба метода понижения размерности работают одинаково."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson_8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
